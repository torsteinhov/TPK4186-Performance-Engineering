{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descendant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import createFromBenchmark, Calculator\n",
    "from math import inf\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from utils import getAllSchedules\n",
    "from math import inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When experimenting with the gradient descendant algorithm, we will use a somewhat large dataset in order to see how the different modifications affects the performance of the algorithm\n",
    "\n",
    "For this experiment we are using the job shop scheduling problem from banchmark_4.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/9w/xmv0pp696b3dv7bw9rk02fb40000gn/T/ipykernel_15418/1579138710.py\", line 6, in <cell line: 6>\n",
      "    all_schedules = getAllSchedules(problem, set_size=3000)\n",
      "  File \"/Users/ane/Projects/Performance Engineering/mini_project_4/utils.py\", line 111, in getAllSchedules\n",
      "    return schedules\n",
      "  File \"/Users/ane/Projects/Performance Engineering/mini_project_4/utils.py\", line 38, in createMixedSchedule\n",
      "IndexError: pop from empty list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1992, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/ane/Projects/Performance Engineering/env/lib/python3.8/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data\n",
    "# Here we are dealing with a somewhat small problem, \n",
    "# this is an ok size for testing the performance for the two different gradient descendant algorithms\n",
    "filename = '/Users/ane/Projects/Performance Engineering/mini_project_4/benchmarks/benchmark_4.txt'\n",
    "problem = createFromBenchmark(filename)\n",
    "all_schedules = getAllSchedules(problem, set_size=3000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that the optimal schedule for this problem has a make span of 112 timeunits\n",
    "iterations = 100\n",
    "best = 109\n",
    "calc = Calculator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descendant 1\n",
    "This is the simplest gradient descendant algorithm that starts with one random schedule  and checks the nearest neighbours in order to find the optimal schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 100 times, best option was found 1 times\n",
      "Average makespan:  185.7\n",
      "Average iterations:  291.69\n"
     ]
    }
   ],
   "source": [
    "count1 = 0\n",
    "total_makespan1 = 0\n",
    "total_iterations1 = 0\n",
    "\n",
    "for i in range(iterations):\n",
    "    best_schedule, makespan, count = calc.gradientDescendant(problem, all_schedules=all_schedules)\n",
    "    total_makespan1 += makespan\n",
    "    total_iterations1 += count\n",
    "    if makespan <= best:\n",
    "        count1 += 1\n",
    "\n",
    "avg_makespan1 = total_makespan1/iterations\n",
    "avg_iterations1 = total_iterations1/iterations\n",
    "print('Out of %s times, best option was found %s times' % (iterations, count1))\n",
    "print('Average makespan: ', avg_makespan1)\n",
    "print('Average iterations: ', avg_iterations1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descendant 2\n",
    "\n",
    "The Gradient Descendant 1 suffers from two drawbacks:  \n",
    " – It may end up in a so-called local minimum, i.e. a schedule that it is better that all\n",
    "schedules of its neighborhood, but not globally optimum. A way to fix this problem\n",
    "consists in starting from different initial schedules.  \n",
    " – Assume at some point of the descent, the current schedule is σ and that the algorithm\n",
    "picks up a better schedule τ in the neighborhood of σ. Then, σ is in the neighborhood of\n",
    "τ , which means that its score (its total processing time) must be recalculated. An obvious\n",
    "waste of computation resources, that will become even more problematic in Section 2.4.  \n",
    "As way to fix this problem consists in memorizing (caching) the schedules for which the\n",
    "calculation of the score has been already performed. Note that this requires to store\n",
    "schedules (and their scores) in a data structure in which insertion and retrieval is efficient.\n",
    "\n",
    "\n",
    "The Gradient Descendant 2 starts from three different inital schedules in order to find the global minimum. The drawback from this is that it will check more schedules and calculate more timespans (which takes time). Hopefully we will save some time because we have memorized already calculated routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations:  100\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Out of 100 times, best option was found 5 times\n",
      "Average makespan:  138.43\n",
      "Average iterations:  877.74\n"
     ]
    }
   ],
   "source": [
    "count2 = 0\n",
    "total_makespan2 = 0\n",
    "total_iterations2 = 0\n",
    "\n",
    "print('Total iterations: ', iterations)\n",
    "for i in range(iterations):\n",
    "    print(i)\n",
    "    best_schedule, makespan, count = calc.gradientDescendant2(problem, all_schedules=all_schedules)\n",
    "    total_makespan2 += makespan\n",
    "    total_iterations2 += count\n",
    "    if makespan <= best:\n",
    "        count2 += 1\n",
    "\n",
    "avg_makespan2 = total_makespan2/iterations\n",
    "avg_iterations2 = total_iterations2/iterations\n",
    "print('Out of %s times, best option was found %s times' % (iterations, count2))\n",
    "print('Average makespan: ', avg_makespan2)\n",
    "print('Average iterations: ', avg_iterations2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the algorithm takes longer to run than the first, but it has a better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descendant 3\n",
    "For this algorithm we are using the Decision Tree Regressor to predict the makespan of a schedule. The reason we are using this method is to avoid calculatibng the total makespan of a schedule that possibly is worse than the best option we have already found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression.ipynb you can see how we ended up choosing this regression model.\n",
    "\n",
    "The dataset we are using to train the algorithm can be found in dataframe_6.csv and consists of about 3000 different possible schedules for the job shop problem from benchmark_4 with the corresponding timespan for each schedule. We decided to save this in a csv  file in order to avoid calculating the timespan for every schedule every time as this takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing dataset\n",
    "dataset = pd.read_csv('../mini_project_4/dataframes/dataframe_6.csv')\n",
    "X = dataset.iloc[:, 0:64]\n",
    "y = dataset.iloc[:, -1]\n",
    "\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# fit the model\n",
    "tree_regressor = DecisionTreeRegressor(random_state = 0)\n",
    "tree_regressor.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 200 times, best option was found 11 times\n",
      "Average makespan:  160.125\n",
      "Average iterations:  381.685\n"
     ]
    }
   ],
   "source": [
    "iterations2 = 200\n",
    "count3 = 0\n",
    "total_makespan3 = 0\n",
    "total_iterations3 = 0\n",
    "best_found = inf\n",
    "\n",
    "for i in range(iterations2):\n",
    "    best_schedule, makespan, count = calc.gradientDescendant3(problem, tree_regressor, all_schedules=all_schedules)\n",
    "    total_makespan3 += makespan\n",
    "    total_iterations3 += count\n",
    "    if makespan <= best:\n",
    "        count3 += 1\n",
    "    if makespan < best_found:\n",
    "        best_found = makespan\n",
    "\n",
    "avg_makespan3 = total_makespan3/iterations2\n",
    "avg_iterations3 = total_iterations3/iterations2\n",
    "print('Out of %s times, best option was found %s times' % (iterations2, count3))\n",
    "print('Average makespan: ', avg_makespan3)\n",
    "print('Average iterations: ', avg_iterations3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descendant 1\n",
      "Out of 100 times, best option was found 1 times\n",
      "Average makespan:  185.7\n",
      "Average iterations:  291.69\n",
      "\n",
      "Gradient Descendant 2\n",
      "Out of 100 times, best option was found 5 times\n",
      "Average makespan:  138.43\n",
      "Average iterations:  877.74\n",
      "\n",
      "Gradient Descendant 3\n",
      "Out of 200 times, best option was found 11 times\n",
      "Best solution found:  109\n",
      "Average makespan:  160.125\n",
      "Average iterations:  381.685\n"
     ]
    }
   ],
   "source": [
    "print('Gradient Descendant 1')\n",
    "print('Out of %s times, best option was found %s times' % (iterations, count1))\n",
    "print('Average makespan: ', avg_makespan1)\n",
    "print('Average iterations: ', avg_iterations1)\n",
    "\n",
    "print('\\nGradient Descendant 2')\n",
    "print('Out of %s times, best option was found %s times' % (iterations, count2))\n",
    "print('Average makespan: ', avg_makespan2)\n",
    "print('Average iterations: ', avg_iterations2)\n",
    "\n",
    "print('\\nGradient Descendant 3')\n",
    "print('Out of %s times, best option was found %s times' % (iterations2, count3))\n",
    "print('Best solution found: ', best_found)\n",
    "print('Average makespan: ', avg_makespan3)\n",
    "print('Average iterations: ', avg_iterations3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the difference between the three algorithms. By comparing the first and the second gradient descendant algorithm, we see that the second one that start from three different initial states takes a lot longer to find the best schedule, but it also finds better schedules than the first algorithm.\n",
    "\n",
    "By implementing a regression model to guess the score of a schedule without actually computing it we see that the running time for the algorithm has decreased a lot. The results are not as good as the Gradient Descendant 2 algorithm, but at the same time it is much better that the original algorithm.\n",
    "\n",
    "A possible reason why the Gradient Descendant 3 algorithm doesn't find the best solution as often as the Gradient Descendant 2 is because we have not found the optimal regression model to use or that the training dataset for the model is too small. \n",
    "\n",
    "Because the  last algorithm is so quick it is also possible to start from maybe 6 different inital states instead of three without having a worse running time that the Gradient Descendant 2 algorithm."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca150290dc79091d5642c1432564df6210ffd8d684dc0da4ee7ce9bf75211ec1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
